**[Feb. 2026]** Our quantization paper is online, entitled [QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals](https://arxiv.org/abs/2602.02581). Our method delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on an RL fine-tuned model!
**[Jan. 2026]** Excited that our [LRMs compression benchmarking and interpreation paper](https://arxiv.org/abs/2504.02010) is accepted by ICLR 2026!
**[Oct. 2025]** Our benchmarking and interpreation on compressed large reasoning models (LRMs) is online, entitled [When Reasoning Meets Compression: Understanding the Effects of LLMs Compression on Large Reasoning Models](https://arxiv.org/abs/2504.02010). We provide analysis on quantized, distilled, and pruned LRMs to decode the effects of compression!
**[Sept. 2025]** Our paper on creating training data for Process Reward Models (PRMs) is online, entitled [Generalizable Process Reward Models via Formally Verified Training Data](https://arxiv.org/abs/2505.15960). Feel free to check it out!
**[June 2025]** I am honored to receive 2024-25 Vice Provost and Dean of the Graduate School Student Persistence Scholarship!
**[Apr. 2025]** Excited that [SiReRAG](https://openreview.net/forum?id=yp95goUAT1) is accepted by ICLR 2025! My collaborators are presenting it in person during Poster Session 1 (#61 at Hall 3 + Hall 2B). I am happy to discuss research on RAG, LLMs compression, and large reasoning models virtually.
**[Apr. 2025]** Our benchmarking paper on compressed large reasoning models (LRMs) is online, entitled [When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks](https://arxiv.org/abs/2504.02010). We provide detailed analysis on quantized, distilled, and pruned reasoning models!
**[Dec. 2024]** Our RAG indexing paper on similar and related corpus contents is online, entitled [SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/pdf/2412.06206). Our paper consistently outperforms current indexing works on multihop datasets!
**[Sept. 2024]** One paper on LLMs as paper reviewers and area chairs has been accepted to EMNLP 2024.
**[Aug. 2024]** One paper on self-correction of LLMs has been accepted to TACL 2024.
**[July 2024]** One paper on error detection benchmark of LLMs has been accepted to COLM 2024.
**[June 2024]** Our survey paper on self-correction of LLMs is online, entitled [When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs](https://arxiv.org/abs/2406.01297). Feel free to check it out!
**[Apr. 2024]** I will join Salesforce AI Research as a Research Intern at Palo Alto, CA in summer 2024! 
**[Apr. 2024]** Our paper on error detection benchmark of LLMs is online, entitled [Evaluating LLMs at Detecting Errors in LLM Responses](https://arxiv.org/abs/2404.03602). Feel free to check it out!
**[Mar. 2024]** Two papers on LLMs compression and LLMs summarization have been accepted to NAACL 2024.
**[Feb. 2024]** Our FaMeSumm paper has been reported by [Penn State News](https://www.psu.edu/news/information-sciences-and-technology/story/improving-efficiency-reliability-ai-medical-summarization/) and other external sites.
**[Feb. 2024]** One paper on chemistry-oriented OCR has been accepted to LREC-COLING 2024.
**[Jan. 2024]** Our survey paper on resource-efficient LLMs is online, entitled [Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models](https://arxiv.org/abs/2401.00625). Feel free to check it out!
**[Dec. 2023]** Presented our [FaMeSumm](https://aclanthology.org/2023.emnlp-main.673/) paper at EMNLP 2023 in Singapore.
**[Nov. 2023]** Our paper on [fair abstractive summarization of diverse perspectives](https://arxiv.org/abs/2311.07884) is online. Feel free to check it out!